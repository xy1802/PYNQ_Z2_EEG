{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import scipy.io as sio\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "np.random.seed(1234)\n",
    "from functools import reduce\n",
    "import math as m\n",
    "\n",
    "import scipy.io\n",
    "#import theano\n",
    "#import theano.tensor as T\n",
    "\n",
    "from scipy.interpolate import griddata\n",
    "from sklearn.preprocessing import scale\n",
    "#from utils import augment_EEG, cart2sph, pol2cart\n",
    "\n",
    "#import lasagne\n",
    "# from lasagne.layers.dnn import Conv2DDNNLayer as ConvLayer\n",
    "#from lasagne.layers import Conv2DLayer, MaxPool2DLayer, InputLayer\n",
    "#from lasagne.layers import DenseLayer, ElemwiseMergeLayer, FlattenLayer\n",
    "#from lasagne.layers import ConcatLayer, ReshapeLayer, get_output_shape\n",
    "#from lasagne.layers import Conv1DLayer, DimshuffleLayer, LSTMLayer, SliceLayer\n",
    "\n",
    "\n",
    "def azim_proj(pos):\n",
    "    \"\"\"\n",
    "    Computes the Azimuthal Equidistant Projection of input point in 3D Cartesian Coordinates.\n",
    "    Imagine a plane being placed against (tangent to) a globe. If\n",
    "    a light source inside the globe projects the graticule onto\n",
    "    the plane the result would be a planar, or azimuthal, map\n",
    "    projection.\n",
    "\n",
    "    :param pos: position in 3D Cartesian coordinates\n",
    "    :return: projected coordinates using Azimuthal Equidistant Projection\n",
    "    \"\"\"\n",
    "    [r, elev, az] = cart2sph(pos[0], pos[1], pos[2])\n",
    "    return pol2cart(az, m.pi / 2 - elev)\n",
    "\n",
    "\n",
    "def gen_images(locs, features, n_gridpoints, normalize=True,\n",
    "               augment=False, pca=False, std_mult=0.1, n_components=2, edgeless=False):\n",
    "    \"\"\"\n",
    "    Generates EEG images given electrode locations in 2D space and multiple feature values for each electrode\n",
    "\n",
    "    :param locs: An array with shape [n_electrodes, 2] containing X, Y\n",
    "                        coordinates for each electrode.\n",
    "    :param features: Feature matrix as [n_samples, n_features]\n",
    "                                Features are as columns.\n",
    "                                Features corresponding to each frequency band are concatenated.\n",
    "                                (alpha1, alpha2, ..., beta1, beta2,...)\n",
    "    :param n_gridpoints: Number of pixels in the output images\n",
    "    :param normalize:   Flag for whether to normalize each band over all samples\n",
    "    :param augment:     Flag for generating augmented images\n",
    "    :param pca:         Flag for PCA based data augmentation\n",
    "    :param std_mult     Multiplier for std of added noise\n",
    "    :param n_components: Number of components in PCA to retain for augmentation\n",
    "    :param edgeless:    If True generates edgeless images by adding artificial channels\n",
    "                        at four corners of the image with value = 0 (default=False).\n",
    "    :return:            Tensor of size [samples, colors, W, H] containing generated\n",
    "                        images.\n",
    "    \"\"\"\n",
    "    feat_array_temp = []\n",
    "    nElectrodes = locs.shape[0]     # Number of electrodes\n",
    "    # Test whether the feature vector length is divisible by number of electrodes\n",
    "    assert features.shape[1] % nElectrodes == 0\n",
    "    n_colors = int(features.shape[1] / nElectrodes)\n",
    "    for c in range(int(n_colors)):\n",
    "        feat_array_temp.append(features[:, c * nElectrodes : nElectrodes * (c+1)])\n",
    "    if augment:\n",
    "        if pca:\n",
    "            for c in range(n_colors):\n",
    "                feat_array_temp[c] = augment_EEG(feat_array_temp[c], std_mult, pca=True, n_components=n_components)\n",
    "        else:\n",
    "            for c in range(n_colors):\n",
    "                feat_array_temp[c] = augment_EEG(feat_array_temp[c], std_mult, pca=False, n_components=n_components)\n",
    "    nSamples = features.shape[0]\n",
    "    # Interpolate the values\n",
    "    grid_x, grid_y = np.mgrid[\n",
    "                     min(locs[:, 0]):max(locs[:, 0]):n_gridpoints*1j,\n",
    "                     min(locs[:, 1]):max(locs[:, 1]):n_gridpoints*1j\n",
    "                     ]\n",
    "    temp_interp = []\n",
    "    for c in range(n_colors):\n",
    "        temp_interp.append(np.zeros([nSamples, n_gridpoints, n_gridpoints]))\n",
    "    # Generate edgeless images\n",
    "    if edgeless:\n",
    "        min_x, min_y = np.min(locs, axis=0)\n",
    "        max_x, max_y = np.max(locs, axis=0)\n",
    "        locs = np.append(locs, np.array([[min_x, min_y], [min_x, max_y],[max_x, min_y],[max_x, max_y]]),axis=0)\n",
    "        for c in range(n_colors):\n",
    "            feat_array_temp[c] = np.append(feat_array_temp[c], np.zeros((nSamples, 4)), axis=1)\n",
    "    # Interpolating\n",
    "    for i in range(nSamples):\n",
    "        for c in range(n_colors):\n",
    "            temp_interp[c][i, :, :] = griddata(locs, feat_array_temp[c][i, :], (grid_x, grid_y),\n",
    "                                    method='cubic', fill_value=np.nan)\n",
    "        print('Interpolating {0}/{1}\\r'.format(i+1, nSamples), end='\\r')\n",
    "    # Normalizing\n",
    "    for c in range(n_colors):\n",
    "        if normalize:\n",
    "            temp_interp[c][~np.isnan(temp_interp[c])] = \\\n",
    "                scale(temp_interp[c][~np.isnan(temp_interp[c])])\n",
    "        temp_interp[c] = np.nan_to_num(temp_interp[c])\n",
    "    return np.swapaxes(np.asarray(temp_interp), 0, 1)     # swap axes to have [samples, colors, W, H]\n",
    "\n",
    "def get_fft(snippet):\n",
    "    Fs = 200.0;  # sampling rate\n",
    "    #Ts = len(snippet)/Fs/Fs; # sampling interval\n",
    "    snippet_time = len(snippet)/Fs\n",
    "    Ts = 0.5/Fs; # sampling interval\n",
    "    t = np.arange(0,snippet_time,Ts) # time vector\n",
    "\n",
    "    # ff = 5;   # frequency of the signal\n",
    "    # y = np.sin(2*np.pi*ff*t)\n",
    "    y = snippet\n",
    "#     print('Ts: ',Ts)\n",
    "#     print(t)\n",
    "#     print(y.shape)\n",
    "    n = len(y) # length of the signal\n",
    "    k = np.arange(n)\n",
    "    T = n/Fs\n",
    "    frq = k/T # two sides frequency range\n",
    "    frq = frq[range(n//2)] # one side frequency range\n",
    "\n",
    "    Y = np.fft.fft(y)/n # fft computing and normalization\n",
    "    Y = Y[range(n//2)]\n",
    "    #Added in: (To remove bias.)\n",
    "    #Y[0] = 0\n",
    "    return frq,abs(Y)\n",
    "\n",
    "def theta_alpha_beta_averages(f,Y):\n",
    "    theta_range = (4,8)\n",
    "    alpha_range = (8,12)\n",
    "    beta_range = (12,40)\n",
    "    theta = Y[(f>theta_range[0]) & (f<=theta_range[1])].mean()\n",
    "    alpha = Y[(f>alpha_range[0]) & (f<=alpha_range[1])].mean()\n",
    "    beta = Y[(f>beta_range[0]) & (f<=beta_range[1])].mean()\n",
    "    return theta, alpha, beta\n",
    "\n",
    "def make_frames(mark,df):\n",
    "    '''\n",
    "    mark: the time label of your dataset\n",
    "    df: all channels in dyour dataset\n",
    "    '''\n",
    "    frames = []\n",
    "    \n",
    "    for i in range(len(mark)-1):\n",
    "        frame = []\n",
    "        for channel in df.columns:\n",
    "            snippet = np.array(df.loc[mark[i]:mark[i+1],int(channel)])\n",
    "            f,Y =  get_fft(snippet)\n",
    "            theta, alpha, beta = theta_alpha_beta_averages(f,Y)\n",
    "            frame.append([theta, alpha, beta])\n",
    "        frames.append(frame)\n",
    "        \n",
    "    return np.array(frames)\n",
    "\n",
    "def TrainTest_Model(model, trainloader, testloader, n_epoch=30, opti='Adam', learning_rate=0.0001, is_cuda=True, print_epoch =5, verbose=False):\n",
    "    net = model()\n",
    "        \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    if opti=='SGD':\n",
    "        optimizer = optim.SGD(net.parameters(), lr=learning_rate)\n",
    "    elif opti =='Adam':\n",
    "        optimizer = optim.Adam(net.parameters(), lr=learning_rate)\n",
    "    else: \n",
    "        print(\"Optimizer: \"+optim+\" not implemented.\")\n",
    "    a = 0\n",
    "    count = 0\n",
    "    for epoch in range(n_epoch):\n",
    "        running_loss = 0.0\n",
    "        evaluation = []\n",
    "        for i, data in enumerate(trainloader, 0):\n",
    "            # get the inputs; data is a list of [inputs, labels]\n",
    "            inputs, labels = data\n",
    "            #a=a+1\n",
    "            \"\"\"if (a<2):\n",
    "                print(inputs)\n",
    "                print(labels)\n",
    "                print(data)\"\"\"\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # forward + backward + optimize\n",
    "            outputs = net(inputs.to(torch.float32))\n",
    "            _, predicted = torch.max(outputs.cpu().data, 1)\n",
    "            \"\"\" if a < 2:\n",
    "                print(predicted)\"\"\"\n",
    "            evaluation.append((predicted==labels).tolist())\n",
    "            loss = criterion(outputs, labels.long())\n",
    "            \"\"\"if a < 2:\n",
    "                print(evaluation)\"\"\"\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        running_loss = running_loss/(i+1)\n",
    "        evaluation = [item for sublist in evaluation for item in sublist]\n",
    "        #print(len(evaluation))\n",
    "        running_acc = sum(evaluation)/len(evaluation)\n",
    "        validation_loss, validation_acc = Test_Model(net, testloader, criterion,True)\n",
    "        count = count+1\n",
    "        if epoch%print_epoch==(print_epoch-1):\n",
    "            print('[%d, %3d]\\tloss: %.3f\\tAccuracy : %.3f\\t\\tval-loss: %.3f\\tval-Accuracy : %.3f' %\n",
    "             (epoch+1, n_epoch, running_loss, running_acc, validation_loss, validation_acc))\n",
    "        if count%10 == 0:\n",
    "            print('epoch:%d \\n loss: %.3f\\tAccuracy : %.3f\\t\\tval-loss: %.3f\\tval-Accuracy : %.3f' %\n",
    "                 (count, running_loss, running_acc, validation_loss,validation_acc))\n",
    "    if verbose:\n",
    "        print('Finished Training \\n loss: %.3f\\tAccuracy : %.3f\\t\\tval-loss: %.3f\\tval-Accuracy : %.3f' %\n",
    "                 (running_loss, running_acc, validation_loss,validation_acc))\n",
    "    torch.save(net.state_dict(), \"BasicCNN_forall_mix.pth\")\n",
    "    return (running_loss, running_acc, validation_loss,validation_acc)\n",
    "\n",
    "from torch.utils.data.dataset import Dataset\n",
    "import torch\n",
    "import scipy.io as sio\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "class EEGImagesDataset(Dataset):\n",
    "    \"\"\"EEGLearn Images Dataset from EEG.\"\"\"\n",
    "    \n",
    "    def __init__(self, label, image):\n",
    "        self.label = label\n",
    "        self.Images = image\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.label)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "        image = self.Images[idx]\n",
    "        label = self.label[idx]\n",
    "        sample = (image, label)\n",
    "        \n",
    "        return sample\n",
    "\n",
    "def Test_Model(net, Testloader, criterion, is_cuda=True):\n",
    "    running_loss = 0.0 \n",
    "    evaluation = []\n",
    "    for i, data in enumerate(Testloader, 0):\n",
    "        input_img, labels = data\n",
    "        input_img = input_img.to(torch.float32)\n",
    "        \"\"\"if is_cuda:\n",
    "            input_img = input_img.cuda()\"\"\"\n",
    "        outputs = net(input_img)\n",
    "        _, predicted = torch.max(outputs.cpu().data, 1)\n",
    "        evaluation.append((predicted==labels).tolist())\n",
    "        loss = criterion(outputs, labels.long())\n",
    "        running_loss += loss.item()\n",
    "    running_loss = running_loss/(i+1)\n",
    "    evaluation = [item for sublist in evaluation for item in sublist]\n",
    "    running_acc = sum(evaluation)/len(evaluation)\n",
    "    return running_loss, running_acc\n",
    "\n",
    "class BasicCNN(nn.Module):\n",
    "    '''\n",
    "    Build the  Mean Basic model performing a classification with CNN \n",
    "\n",
    "    param input_image: list of EEG image [batch_size, n_window, n_channel, h, w]\n",
    "    param kernel: kernel size used for the convolutional layers\n",
    "    param stride: stride apply during the convolutions\n",
    "    param padding: padding used during the convolutions\n",
    "    param max_kernel: kernel used for the maxpooling steps\n",
    "    param n_classes: number of classes\n",
    "    return x: output of the last layers after the log softmax\n",
    "    '''\n",
    "\n",
    "    def __init__(self, input_image=torch.zeros(1, 3, 32, 32), kernel=(3, 3), stride=1, padding=1, max_kernel=(2, 2),\n",
    "                 n_classes=4):\n",
    "        super(BasicCNN, self).__init__()\n",
    "\n",
    "        n_channel = input_image.shape[1]\n",
    "\n",
    "        self.conv1 = nn.Conv2d(n_channel, 32, kernel, stride=stride, padding=padding)\n",
    "        self.conv2 = nn.Conv2d(32, 32, kernel, stride=stride, padding=padding)\n",
    "        self.conv3 = nn.Conv2d(32, 32, kernel, stride=stride, padding=padding)\n",
    "        self.conv4 = nn.Conv2d(32, 32, kernel, stride=stride, padding=padding)\n",
    "        self.pool1 = nn.MaxPool2d(max_kernel)\n",
    "        self.conv5 = nn.Conv2d(32, 64, kernel, stride=stride, padding=padding)\n",
    "        self.conv6 = nn.Conv2d(64, 64, kernel, stride=stride, padding=padding)\n",
    "        self.conv7 = nn.Conv2d(64, 128, kernel, stride=stride, padding=padding)\n",
    "\n",
    "        self.pool = nn.MaxPool2d((1, 1))\n",
    "        self.drop = nn.Dropout(p=0.5)\n",
    "\n",
    "        self.fc1 = nn.Linear(2048, 512)\n",
    "        self.fc2 = nn.Linear(512, n_classes)\n",
    "        self.max = nn.LogSoftmax()\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size = x.shape[0]\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = F.relu(self.conv4(x))\n",
    "        x = self.pool1(x)\n",
    "        x = F.relu(self.conv5(x))\n",
    "        x = F.relu(self.conv6(x))\n",
    "        x = self.pool1(x)\n",
    "        x = F.relu(self.conv7(x))\n",
    "        x = self.pool1(x)\n",
    "        x = x.reshape(x.shape[0], x.shape[1], -1)\n",
    "        x = self.pool(x)\n",
    "        x = x.reshape(x.shape[0], -1)\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.max(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(47001, 62)\n",
      "(46601, 62)\n",
      "(41201, 62)\n"
     ]
    }
   ],
   "source": [
    "dataset = sio.loadmat(\"SEED/Preprocessed_EEG/1_20131027.mat\")\n",
    "features_struct = sio.loadmat(\"SEED/Preprocessed_EEG/1_20131027.mat\")\n",
    "features_1 = features_struct['djc_eeg1']\n",
    "dfdata_1 = pd.DataFrame(features_1)\n",
    "df_1 = dfdata_1.T\n",
    "print(df_1.shape)\n",
    "features_2 = features_struct['djc_eeg2']\n",
    "dfdata_2 = pd.DataFrame(features_2)\n",
    "df_2 = dfdata_2.T\n",
    "print(df_2.shape)\n",
    "features_3 = features_struct['djc_eeg3']\n",
    "dfdata_3 = pd.DataFrame(features_3)\n",
    "df_3 = dfdata_3.T\n",
    "print(df_3.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "mark_a =np.linspace(0,47000,471)\n",
    "mark_b =np.linspace(0,46600,467)\n",
    "mark_c =np.linspace(0,41200,413)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_frames(mark,df):\n",
    "    '''\n",
    "    mark: the time label of your dataset\n",
    "    df: all channels in dyour dataset\n",
    "    '''\n",
    "    frames = []\n",
    "    \n",
    "    for i in range(len(mark)-1):\n",
    "        frame = []\n",
    "        for channel in df.columns:\n",
    "            snippet = np.array(df.loc[mark[i]:mark[i+1],int(channel)])\n",
    "            f,Y =  get_fft(snippet)\n",
    "            theta, alpha, beta = theta_alpha_beta_averages(f,Y)\n",
    "            frame.append([theta, alpha, beta])\n",
    "        frames.append(frame)\n",
    "        \n",
    "    return np.array(frames)\n",
    "\n",
    "X = make_frames(mark_a,df_1)\n",
    "Y = make_frames(mark_b,df_2)\n",
    "Z = make_frames(mark_c,df_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interpolating 470/470nterpolating 11/470Interpolating 16/470Interpolating 21/470Interpolating 26/470Interpolating 31/470Interpolating 36/470Interpolating 41/470Interpolating 46/470Interpolating 51/470Interpolating 56/470Interpolating 61/470Interpolating 66/470Interpolating 71/470Interpolating 76/470Interpolating 81/470Interpolating 86/470Interpolating 91/470Interpolating 96/470Interpolating 101/470Interpolating 106/470Interpolating 111/470Interpolating 116/470Interpolating 121/470Interpolating 126/470Interpolating 131/470Interpolating 136/470Interpolating 141/470Interpolating 146/470Interpolating 151/470Interpolating 156/470Interpolating 161/470Interpolating 166/470Interpolating 171/470Interpolating 176/470Interpolating 181/470Interpolating 186/470Interpolating 189/470Interpolating 192/470Interpolating 195/470Interpolating 197/470Interpolating 202/470Interpolating 207/470Interpolating 212/470Interpolating 217/470Interpolating 222/470Interpolating 227/470Interpolating 232/470Interpolating 237/470Interpolating 242/470Interpolating 247/470Interpolating 252/470Interpolating 257/470Interpolating 262/470Interpolating 267/470Interpolating 272/470Interpolating 277/470Interpolating 282/470Interpolating 287/470Interpolating 292/470Interpolating 297/470Interpolating 302/470Interpolating 307/470Interpolating 312/470Interpolating 317/470Interpolating 322/470Interpolating 327/470Interpolating 332/470Interpolating 337/470Interpolating 342/470Interpolating 347/470Interpolating 352/470Interpolating 357/470Interpolating 362/470Interpolating 367/470Interpolating 372/470Interpolating 377/470Interpolating 382/470Interpolating 387/470Interpolating 392/470Interpolating 397/470Interpolating 402/470Interpolating 407/470Interpolating 412/470Interpolating 417/470Interpolating 421/470Interpolating 426/470Interpolating 431/470Interpolating 436/470Interpolating 441/470Interpolating 446/470Interpolating 451/470Interpolating 456/470Interpolating 461/470Interpolating 466/470"
     ]
    }
   ],
   "source": [
    "X_1 = X.reshape(X.shape[0],X.shape[1]*X.shape[2])\n",
    "eeg_locs = sio.loadmat(\"locs_seed.mat\")\n",
    "locs_2d =eeg_locs[\"locs\"]\n",
    "images_a = gen_images(locs_2d,X_1, 32, normalize=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interpolating 465/466nterpolating 11/466Interpolating 16/466Interpolating 21/466Interpolating 26/466Interpolating 31/466Interpolating 36/466Interpolating 41/466Interpolating 46/466Interpolating 51/466Interpolating 56/466Interpolating 61/466Interpolating 66/466Interpolating 71/466Interpolating 76/466Interpolating 81/466Interpolating 86/466Interpolating 91/466Interpolating 96/466Interpolating 101/466Interpolating 106/466Interpolating 111/466Interpolating 116/466Interpolating 121/466Interpolating 126/466Interpolating 131/466Interpolating 136/466Interpolating 141/466Interpolating 146/466Interpolating 151/466Interpolating 156/466Interpolating 161/466Interpolating 166/466Interpolating 171/466Interpolating 176/466Interpolating 181/466Interpolating 186/466Interpolating 191/466Interpolating 196/466Interpolating 201/466Interpolating 206/466Interpolating 211/466Interpolating 216/466Interpolating 221/466Interpolating 226/466Interpolating 231/466Interpolating 236/466Interpolating 241/466Interpolating 246/466Interpolating 251/466Interpolating 256/466Interpolating 261/466Interpolating 266/466Interpolating 271/466Interpolating 276/466Interpolating 281/466Interpolating 286/466Interpolating 291/466Interpolating 296/466Interpolating 301/466Interpolating 306/466Interpolating 311/466Interpolating 316/466Interpolating 321/466Interpolating 326/466Interpolating 331/466Interpolating 336/466Interpolating 341/466Interpolating 346/466Interpolating 351/466Interpolating 356/466Interpolating 361/466Interpolating 366/466Interpolating 371/466Interpolating 376/466Interpolating 381/466Interpolating 386/466Interpolating 391/466Interpolating 396/466Interpolating 401/466Interpolating 406/466Interpolating 411/466Interpolating 416/466Interpolating 421/466Interpolating 426/466Interpolating 431/466Interpolating 436/466Interpolating 441/466Interpolating 446/466Interpolating 451/466Interpolating 456/466Interpolating 461/466Interpolating 466/466"
     ]
    }
   ],
   "source": [
    "Y_1 = Y.reshape(Y.shape[0],Y.shape[1]*Y.shape[2])\n",
    "images_b = gen_images(locs_2d,Y_1, 32, normalize=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interpolating 412/412nterpolating 11/412Interpolating 16/412Interpolating 21/412Interpolating 26/412Interpolating 31/412Interpolating 36/412Interpolating 41/412Interpolating 46/412Interpolating 51/412Interpolating 56/412Interpolating 61/412Interpolating 66/412Interpolating 71/412Interpolating 76/412Interpolating 81/412Interpolating 86/412Interpolating 91/412Interpolating 96/412Interpolating 101/412Interpolating 106/412Interpolating 111/412Interpolating 116/412Interpolating 121/412Interpolating 126/412Interpolating 131/412Interpolating 136/412Interpolating 141/412Interpolating 146/412Interpolating 151/412Interpolating 155/412Interpolating 158/412Interpolating 160/412Interpolating 165/412Interpolating 170/412Interpolating 175/412Interpolating 180/412Interpolating 185/412Interpolating 190/412Interpolating 195/412Interpolating 200/412Interpolating 205/412Interpolating 210/412Interpolating 215/412Interpolating 220/412Interpolating 225/412Interpolating 230/412Interpolating 235/412Interpolating 240/412Interpolating 245/412Interpolating 250/412Interpolating 255/412Interpolating 260/412Interpolating 265/412Interpolating 270/412Interpolating 275/412Interpolating 280/412Interpolating 285/412Interpolating 290/412Interpolating 295/412Interpolating 300/412Interpolating 305/412Interpolating 310/412Interpolating 315/412Interpolating 320/412Interpolating 325/412Interpolating 330/412Interpolating 335/412Interpolating 340/412Interpolating 345/412Interpolating 350/412Interpolating 355/412Interpolating 360/412Interpolating 365/412Interpolating 370/412Interpolating 375/412Interpolating 380/412Interpolating 385/412Interpolating 390/412Interpolating 395/412Interpolating 400/412Interpolating 405/412Interpolating 410/412"
     ]
    }
   ],
   "source": [
    "Z_1 = Z.reshape(Z.shape[0],Z.shape[1]*Z.shape[2])\n",
    "images_c = gen_images(locs_2d,Z_1, 32, normalize=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n"
     ]
    }
   ],
   "source": [
    "label = []\n",
    "for i in range(470):\n",
    "    label.append(0)\n",
    "for i in range(460):\n",
    "    label.append(1)\n",
    "for i in range(412):\n",
    "    label.append(2)\n",
    "print(label)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'images' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-2b0681aeae8b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'images' is not defined"
     ]
    }
   ],
   "source": [
    "print(images.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1348, 3, 32, 32)\n"
     ]
    }
   ],
   "source": [
    "images = np.concatenate((images_a,images_b,images_c),axis=0)\n",
    "\n",
    "print(images.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\ANACONDA\\envs\\PYNQ\\lib\\site-packages\\ipykernel_launcher.py:312: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:10 \n",
      " loss: 0.697\tAccuracy : 0.638\t\tval-loss: 0.883\tval-Accuracy : 0.481\n",
      "epoch:20 \n",
      " loss: 0.354\tAccuracy : 0.861\t\tval-loss: 0.476\tval-Accuracy : 0.784\n",
      "epoch:30 \n",
      " loss: 0.272\tAccuracy : 0.886\t\tval-loss: 0.437\tval-Accuracy : 0.821\n",
      "epoch:40 \n",
      " loss: 0.247\tAccuracy : 0.900\t\tval-loss: 0.412\tval-Accuracy : 0.840\n",
      "epoch:50 \n",
      " loss: 0.213\tAccuracy : 0.914\t\tval-loss: 0.379\tval-Accuracy : 0.862\n",
      "epoch:60 \n",
      " loss: 0.191\tAccuracy : 0.920\t\tval-loss: 0.366\tval-Accuracy : 0.877\n",
      "epoch:70 \n",
      " loss: 0.177\tAccuracy : 0.928\t\tval-loss: 0.377\tval-Accuracy : 0.869\n",
      "epoch:80 \n",
      " loss: 0.152\tAccuracy : 0.939\t\tval-loss: 0.364\tval-Accuracy : 0.881\n",
      "epoch:90 \n",
      " loss: 0.136\tAccuracy : 0.947\t\tval-loss: 0.386\tval-Accuracy : 0.877\n",
      "epoch:100 \n",
      " loss: 0.121\tAccuracy : 0.950\t\tval-loss: 0.375\tval-Accuracy : 0.877\n",
      "epoch:110 \n",
      " loss: 0.109\tAccuracy : 0.952\t\tval-loss: 0.429\tval-Accuracy : 0.873\n",
      "epoch:120 \n",
      " loss: 0.092\tAccuracy : 0.966\t\tval-loss: 0.385\tval-Accuracy : 0.892\n",
      "epoch:130 \n",
      " loss: 0.076\tAccuracy : 0.973\t\tval-loss: 0.364\tval-Accuracy : 0.888\n",
      "epoch:140 \n",
      " loss: 0.088\tAccuracy : 0.961\t\tval-loss: 0.423\tval-Accuracy : 0.873\n",
      "epoch:150 \n",
      " loss: 0.059\tAccuracy : 0.980\t\tval-loss: 0.399\tval-Accuracy : 0.888\n",
      "epoch:160 \n",
      " loss: 0.119\tAccuracy : 0.950\t\tval-loss: 0.622\tval-Accuracy : 0.866\n",
      "epoch:170 \n",
      " loss: 0.064\tAccuracy : 0.978\t\tval-loss: 0.463\tval-Accuracy : 0.869\n",
      "epoch:180 \n",
      " loss: 0.076\tAccuracy : 0.964\t\tval-loss: 0.415\tval-Accuracy : 0.873\n",
      "epoch:190 \n",
      " loss: 0.078\tAccuracy : 0.962\t\tval-loss: 0.599\tval-Accuracy : 0.866\n",
      "epoch:200 \n",
      " loss: 0.088\tAccuracy : 0.966\t\tval-loss: 0.746\tval-Accuracy : 0.858\n",
      "epoch:210 \n",
      " loss: 0.073\tAccuracy : 0.966\t\tval-loss: 0.543\tval-Accuracy : 0.858\n",
      "epoch:220 \n",
      " loss: 0.061\tAccuracy : 0.981\t\tval-loss: 0.463\tval-Accuracy : 0.877\n",
      "epoch:230 \n",
      " loss: 0.021\tAccuracy : 0.997\t\tval-loss: 0.477\tval-Accuracy : 0.881\n",
      "epoch:240 \n",
      " loss: 0.015\tAccuracy : 0.998\t\tval-loss: 0.529\tval-Accuracy : 0.881\n",
      "epoch:250 \n",
      " loss: 0.011\tAccuracy : 0.999\t\tval-loss: 0.563\tval-Accuracy : 0.884\n",
      "epoch:260 \n",
      " loss: 0.030\tAccuracy : 0.990\t\tval-loss: 0.804\tval-Accuracy : 0.873\n",
      "epoch:270 \n",
      " loss: 0.088\tAccuracy : 0.966\t\tval-loss: 0.586\tval-Accuracy : 0.892\n",
      "epoch:280 \n",
      " loss: 0.016\tAccuracy : 0.997\t\tval-loss: 0.522\tval-Accuracy : 0.881\n",
      "epoch:290 \n",
      " loss: 0.014\tAccuracy : 0.997\t\tval-loss: 0.578\tval-Accuracy : 0.888\n",
      "epoch:300 \n",
      " loss: 0.014\tAccuracy : 0.998\t\tval-loss: 0.597\tval-Accuracy : 0.884\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "Begin Training for all Patient \n",
      "End Training with \t loss: 0.014\tAccuracy : 0.998\t\tval-loss: 0.597\tval-Accuracy : 0.884\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import scipy.io as sio\n",
    "import torch\n",
    "import os\n",
    "from os import path\n",
    "\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "from torch.utils.data.dataset import Dataset\n",
    "from torch.utils.data import DataLoader,random_split\n",
    "\n",
    "#from Utils import *\n",
    "#from Models import *\n",
    "\n",
    "torch.manual_seed(1234)\n",
    "np.random.seed(1234)\n",
    "\n",
    "\n",
    "# Introduction: training a simple CNN with the mean of the images.\n",
    "train_part = 0.8\n",
    "test_part = 0.2\n",
    "\n",
    "batch_size = 64\n",
    "\n",
    "n_epoch = 300\n",
    "n_rep = 1\n",
    "Result = []\n",
    "for r in range(n_rep):\n",
    "    EEG = EEGImagesDataset(label=label, image=images)\n",
    "    lengths = [int(len(EEG) * train_part), int(len(EEG) * test_part)]\n",
    "    if sum(lengths) != len(EEG):\n",
    "        lengths[0] = lengths[0] + 1\n",
    "    Train, Test = random_split(EEG, lengths)\n",
    "    Trainloader = DataLoader(Train, batch_size=batch_size)\n",
    "    Testloader = DataLoader(Test, batch_size=batch_size)\n",
    "    res = TrainTest_Model(BasicCNN, Trainloader, Testloader, n_epoch=n_epoch, learning_rate=0.0001, print_epoch=-1,\n",
    "                              opti='Adam')\n",
    "    Result.append(res)\n",
    "#sio.savemat(\"Res_Basic_Patient\"+\"all\"+\".mat\", {\"res\":Result})\n",
    "Result = np.mean(Result, axis=0)\n",
    "print ('-'*100)\n",
    "print('\\nBegin Training for all Patient ')\n",
    "print('End Training with \\t loss: %.3f\\tAccuracy : %.3f\\t\\tval-loss: %.3f\\tval-Accuracy : %.3f' %\n",
    "    (Result[0], Result[1], Result[2], Result[3]))\n",
    "print('\\n'+'-'*100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for BasicCNN:\n\tsize mismatch for fc2.weight: copying a param with shape torch.Size([2, 512]) from checkpoint, the shape in current model is torch.Size([4, 512]).\n\tsize mismatch for fc2.bias: copying a param with shape torch.Size([2]) from checkpoint, the shape in current model is torch.Size([4]).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-1986fab0c6c5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;31m# print(model)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;31m# model = torch.load(\"BasicCNN.pkl\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"BasicCNN_forall_mix.txt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;31m# model = model().cuda()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/xilinx/pytorch/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mload_state_dict\u001b[0;34m(self, state_dict, strict)\u001b[0m\n\u001b[1;32m    843\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_msgs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    844\u001b[0m             raise RuntimeError('Error(s) in loading state_dict for {}:\\n\\t{}'.format(\n\u001b[0;32m--> 845\u001b[0;31m                                self.__class__.__name__, \"\\n\\t\".join(error_msgs)))\n\u001b[0m\u001b[1;32m    846\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_IncompatibleKeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmissing_keys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munexpected_keys\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    847\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for BasicCNN:\n\tsize mismatch for fc2.weight: copying a param with shape torch.Size([2, 512]) from checkpoint, the shape in current model is torch.Size([4, 512]).\n\tsize mismatch for fc2.bias: copying a param with shape torch.Size([2]) from checkpoint, the shape in current model is torch.Size([4])."
     ]
    }
   ],
   "source": [
    "import torchvision as tv\n",
    "import torchvision.transforms as transforms\n",
    "import torch\n",
    "from PIL import Image\n",
    "import scipy.io as sio\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "#i = 15\n",
    "model = BasicCNN()\n",
    "# print(model)\n",
    "# model = torch.load(\"BasicCNN.pkl\")\n",
    "model.load_state_dict(torch.load(\"BasicCNN_forall_mix.txt\"))\n",
    "# model = model().cuda()\n",
    "model.eval()\n",
    "torch.no_grad()\n",
    "predict = []\n",
    "# img_ = image.to(device)\n",
    "\n",
    "for i in range (10):\n",
    "    image = images[i:i+1]\n",
    "    outputs = model(torch.tensor(image).to(torch.float32))\n",
    "    #print(outputs.data)\n",
    "    _, predicted = torch.max(outputs.data, 1)\n",
    "    predict.append(predicted.item())\n",
    "print(predict)\n",
    "print(len(predict))\n",
    "print(\"all the labels of this patient\")\n",
    "print(label_a)\n",
    "count = 0\n",
    "for i in range(10):\n",
    "    if predict[i]==label_a[i]:\n",
    "        count= count+1\n",
    "print(count)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "image_1 = images[0]*10\n",
    "plt.imshow((images[0]*10).astype(np.uint8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
